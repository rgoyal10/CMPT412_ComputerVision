{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "lab3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6444c5fc898f4caf860c0470b5f06035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c1c45ea9a7347fd9e280fd2a2959ce7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0d5b5a06a3dc490e9d909c8e8fe7a0dc",
              "IPY_MODEL_2e537c36ce314ddeb21fe7b1b25e9ca9",
              "IPY_MODEL_d29ea9d5cda7434795bb58f62c930722"
            ]
          }
        },
        "1c1c45ea9a7347fd9e280fd2a2959ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0d5b5a06a3dc490e9d909c8e8fe7a0dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_91aa4b0d63e84c7ba75c87c28ea8226c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_366734c9a7d846449155f91a82a87602"
          }
        },
        "2e537c36ce314ddeb21fe7b1b25e9ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_25e9c0338bb34e9ab0f39cb314671a56",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 837,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 837,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f5d8649f2d7427cb4934aaa60187e22"
          }
        },
        "d29ea9d5cda7434795bb58f62c930722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c3069fe2f66840179083bdb0debe32d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 837/837 [00:23&lt;00:00, 37.32it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_965c355b65c0408cbc5bab6138c5fcb2"
          }
        },
        "91aa4b0d63e84c7ba75c87c28ea8226c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "366734c9a7d846449155f91a82a87602": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "25e9c0338bb34e9ab0f39cb314671a56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f5d8649f2d7427cb4934aaa60187e22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c3069fe2f66840179083bdb0debe32d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "965c355b65c0408cbc5bab6138c5fcb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7b3925afd1744aa6b98818290fbcc7aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ae7dba3036dd45968e336f584911dcff",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57951810118e4bb5a25a9378de3e3918",
              "IPY_MODEL_f1c7a470ad8b4cad92b3762885936e9f",
              "IPY_MODEL_6eb570df63fe41bbb3c903b8aeeb6cae"
            ]
          }
        },
        "ae7dba3036dd45968e336f584911dcff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57951810118e4bb5a25a9378de3e3918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6703f8e19984d6b97d64f625e53183f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d67a4a8ae01046b1a954a4dc349fa43d"
          }
        },
        "f1c7a470ad8b4cad92b3762885936e9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ed300f57efdd4ab7833e3b6ebf034b6e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 168,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 168,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5ed0bba00fd140dc96c4683f8e75fa9a"
          }
        },
        "6eb570df63fe41bbb3c903b8aeeb6cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cfc861d64ba745d8bdeb87594d75cf81",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 168/168 [05:08&lt;00:00,  1.78s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_80fb3cc6110d4f68878eb434f5b2877c"
          }
        },
        "c6703f8e19984d6b97d64f625e53183f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d67a4a8ae01046b1a954a4dc349fa43d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed300f57efdd4ab7833e3b6ebf034b6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5ed0bba00fd140dc96c4683f8e75fa9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cfc861d64ba745d8bdeb87594d75cf81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "80fb3cc6110d4f68878eb434f5b2877c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a02398ad82134f08807f6ea242b4c79a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9c027802ae8f4bc6a6d2b018db48bc0d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8f61f0f140494de194ac51e71d0c05b0",
              "IPY_MODEL_92526b7a9d514ed0bafa928c4769fb1c",
              "IPY_MODEL_ea31cc7899b94be4b648ae064838b684"
            ]
          }
        },
        "9c027802ae8f4bc6a6d2b018db48bc0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8f61f0f140494de194ac51e71d0c05b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c6a6d527d2f34885b90aa4348dc77ea4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53682e77e13d4691a39aad2174c8d152"
          }
        },
        "92526b7a9d514ed0bafa928c4769fb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_31ebe21e545844db938852cc759e2de8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 30,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 30,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9bfdd9f763eb4d24b995649046f294ca"
          }
        },
        "ea31cc7899b94be4b648ae064838b684": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9434093f4528454f95910344b6f5e52c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 30/30 [01:07&lt;00:00,  1.18s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_49ceffab110d41cf854975f6e062965b"
          }
        },
        "c6a6d527d2f34885b90aa4348dc77ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53682e77e13d4691a39aad2174c8d152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31ebe21e545844db938852cc759e2de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9bfdd9f763eb4d24b995649046f294ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9434093f4528454f95910344b6f5e52c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "49ceffab110d41cf854975f6e062965b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40ca5847e20b4068b635db32f77d2e3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_145638c4a6414cec874831ed057ae549",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_080e286c79b644c2a74a392791e66a58",
              "IPY_MODEL_fdad7ca005ce43f5a0190a0eec89e533",
              "IPY_MODEL_3bc0f535197043009280ce9a119cac3b"
            ]
          }
        },
        "145638c4a6414cec874831ed057ae549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "080e286c79b644c2a74a392791e66a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5f3693a1980b494bb67b44f67761d4e3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2475cc85c4c945c085f22b4774bec488"
          }
        },
        "fdad7ca005ce43f5a0190a0eec89e533": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9616236938da4a7da9dbb8f055173cb1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 72,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 72,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f43b2567dbc84be8822819b0c07405f7"
          }
        },
        "3bc0f535197043009280ce9a119cac3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b553a63650d242fd99966453416b2702",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 72/72 [02:46&lt;00:00,  2.60s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5f3f9f477b7f4da29ea6291dd81afcdb"
          }
        },
        "5f3693a1980b494bb67b44f67761d4e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2475cc85c4c945c085f22b4774bec488": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9616236938da4a7da9dbb8f055173cb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f43b2567dbc84be8822819b0c07405f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b553a63650d242fd99966453416b2702": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5f3f9f477b7f4da29ea6291dd81afcdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHnVupBBn9eR"
      },
      "source": [
        "\n",
        "# Assignment 3\n",
        "\n",
        "This is a template notebook for Assignment 3.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM54r6jlKTII"
      },
      "source": [
        "## Install dependencies and initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_FzH13EjseR"
      },
      "source": [
        "# install dependencies: \n",
        "!pip install pyyaml==5.1 pycocotools>=2.0.1z\n",
        "# !pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n",
        "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqAUb5Y12qXt"
      },
      "source": [
        "!pwd # shows current directory\n",
        "!ls  # shows all files in this directory\n",
        "!nvidia-smi # shows the specs and the current status of the allocated GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-i4hmGYk1dL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4eabc15-5b0f-432a-c42b-d4f4d3d51ec0"
      },
      "source": [
        "# import some common libraries\n",
        "from google.colab.patches import cv2_imshow\n",
        "from sklearn.metrics import jaccard_score\n",
        "from PIL import Image, ImageDraw\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "import json\n",
        "import cv2\n",
        "import csv\n",
        "import os\n",
        "from glob import glob\n",
        "\n",
        "\n",
        "# import some common pytorch utilities\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# import some common detectron2 utilities\n",
        "import detectron2\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.structures import BoxMode\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.logger import setup_logger\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import build_detection_test_loader\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "setup_logger()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Logger detectron2 (DEBUG)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUA_j6AF1L5Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b4ff491-dd20-4770-f6db-373f34535e06"
      },
      "source": [
        "# Make sure that GPU is available for your notebook. \n",
        "# Otherwise, you need to update the settungs in Runtime -> Change runtime type -> Hardware accelerator\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyAvNCJMmvFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e813577e-f972-4272-af5d-5813f0c3ab2e"
      },
      "source": [
        "# You need to mount your google drive in order to load the data:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Put all the corresponding data files in a data folder and put the data folder in a same directory with this notebook.\n",
        "# Also create an output directory for your files such as the trained models and the output images."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Di_fgL4HSv"
      },
      "source": [
        "# Define the location of current directory, which should contain data/train, data/test, and data/train.json.\n",
        "# TODO: approx 1 line\n",
        "BASE_DIR = '/content/drive/My Drive/CMPT_CV_lab3'\n",
        "OUTPUT_DIR = '{}/output'.format(BASE_DIR)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4gID50K03a"
      },
      "source": [
        "### Part 1: Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRV-KFJzlur4"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq9GY37ml1kr"
      },
      "source": [
        "'''\n",
        "# This function should return a list of data samples in which each sample is a dictionary. \n",
        "# Make sure to select the correct bbox_mode for the data\n",
        "# For the test data, you only have access to the images, therefore, the annotations should be empty.\n",
        "# Other values could be obtained from the image files.\n",
        "# TODO: approx 35 lines\n",
        "'''\n",
        "\n",
        "def split_data(set_name):\n",
        "  data_dirs = '{}/data/'.format(BASE_DIR)\n",
        " \n",
        "  data_dirs_json = '{}/data/train.json'.format(BASE_DIR)    #data.train.json\n",
        "  with open(data_dirs_json) as f:\n",
        "        imgs_anns = json.load(f)\n",
        "\n",
        "  input_path = os.path.join(data_dirs,set_name) #data/set_name\n",
        "  dataset = {}\n",
        "\n",
        "  for idx, v in enumerate(imgs_anns):\n",
        "    # print(idx)\n",
        "    # print(v)\n",
        "    record = {}\n",
        "    filename = os.path.join(input_path, v[\"file_name\"])\n",
        "    obj1 = {\n",
        "        \"bbox\": v['bbox'],\n",
        "        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "        \"segmentation\": v['segmentation'],\n",
        "        \"category_id\": 0,\n",
        "        \"iscrowd\" : v[\"iscrowd\"]\n",
        "    }\n",
        "\n",
        "    obj2 = {\n",
        "        \"id\" : v[\"id\"],\n",
        "        \"category_id\": v[\"category_id\"],\n",
        "        \"iscrowd\" : v[\"iscrowd\"]\n",
        "    }\n",
        "\n",
        "    if not filename in dataset:\n",
        "      height, width = cv2.imread(filename).shape[:2]\n",
        "      record[\"file_name\"] = filename\n",
        "      record[\"image_id\"] = idx\n",
        "      record[\"height\"] = height\n",
        "      record[\"width\"] = width\n",
        "\n",
        "      record[\"annotations\"] = []\n",
        "      record[\"segments_info\"] = []\n",
        "      dataset[filename] = record\n",
        "\n",
        "    dataset[filename]['annotations'].append(obj1)\n",
        "    dataset[filename]['segments_info'].append(obj2)\n",
        "    # print(dataset[filename])\n",
        "\n",
        "  # print(dataset)\n",
        "  dataset = np.array(list(dataset.values()))\n",
        "  np.random.shuffle(dataset)\n",
        "  training, validation = dataset[:168], dataset[168:]\n",
        "\n",
        "  test_data = []\n",
        "  for filename in (glob(f'{BASE_DIR}/data/test/*')):\n",
        "    record = {}\n",
        "    height, width = cv2.imread(filename).shape[:2]\n",
        "    record[\"file_name\"] = filename\n",
        "    record[\"height\"] = height\n",
        "    record[\"width\"] = width\n",
        "    record[\"annotations\"] = []\n",
        "    test_data.append(record)\n",
        "  return training, validation, test_data\n",
        "\n",
        "\n",
        "training_set, validation_set, test_set = split_data(\"train\")\n",
        "\n",
        "def get_detection_data(set_name):\n",
        "  if set_name == \"train\":\n",
        "    if type(training_set) is not list:\n",
        "      return training_set.tolist()\n",
        "    else:\n",
        "      return training_set\n",
        "  elif set_name == \"test\":\n",
        "    return test_set\n",
        "  else:\n",
        "    if type(validation_set) is not list:\n",
        "      return validation_set.tolist()\n",
        "    else:\n",
        "      return validation_set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3T1EMNZ3x8WV"
      },
      "source": [
        "# train = get_detection_data(\"train\")\n",
        "# print(type(train))\n",
        "# print(len(train))\n",
        "# print(train[1])\n",
        "# print(train[1]['annotations'][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7f2F1iv9V0Z"
      },
      "source": [
        "# val = get_detection_data(\"val\")\n",
        "# print(type(val))\n",
        "# print(len(val))\n",
        "# print(val[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCRczcbt9V3O"
      },
      "source": [
        "# test = get_detection_data(\"test\")\n",
        "# print(type(test))\n",
        "# print(len(test))\n",
        "# print(test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCH-2mWxDVVu"
      },
      "source": [
        "'''\n",
        "# Remember to add your dataset to DatasetCatalog and MetadataCatalog\n",
        "# Consdier \"data_detection_train\" and \"data_detection_test\" for registration\n",
        "# You can also add an optional \"data_detection_val\" for your validation by spliting the training data\n",
        "# TODO: approx 5 lines\n",
        "'''\n",
        "\n",
        "for d in [\"train\", \"val\",\"test\"]:\n",
        "    # print(d)\n",
        "    DatasetCatalog.register(\"data_detection_\" + d, lambda d=d: get_detection_data(d))\n",
        "    MetadataCatalog.get(\"data_detection_\" + d).set(thing_classes=[\"planes\"])\n",
        "planes_metadata = MetadataCatalog.get(\"data_detection_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNSdXCL_DVAz"
      },
      "source": [
        "'''\n",
        "# Visualize some samples using Visualizer to make sure that the function works correctly\n",
        "# TODO: approx 5 lines\n",
        "'''\n",
        "\n",
        "dataset_dicts = get_detection_data(\"train\")\n",
        "for d in random.sample(dataset_dicts, 3):\n",
        "    # print(d)\n",
        "    img = cv2.imread(d[\"file_name\"])\n",
        "    visualizer = Visualizer(img[:, :, ::-1], metadata=planes_metadata, scale=0.5)\n",
        "    # print(d.values())\n",
        "    out = visualizer.draw_dataset_dict(d)\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uM1thbN-ntjI"
      },
      "source": [
        "### Set Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUjkwRsOn1O0"
      },
      "source": [
        "'''\n",
        "# Set the configs for the detection part in here.\n",
        "# TODO: approx 15 lines\n",
        "'''\n",
        "cfg = get_cfg()\n",
        "cfg.OUTPUT_DIR = \"{}/output/\".format(BASE_DIR)\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"data_detection_train\")\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 1000 #500   \n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4rql8pNokE4"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d3KxiHO_0gb"
      },
      "source": [
        "'''\n",
        "# Create a DefaultTrainer using the above config and train the model\n",
        "# TODO: approx 5 lines\n",
        "'''\n",
        "\n",
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRVEiICco3SV"
      },
      "source": [
        "### Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M0mosf8MUph"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir drive/My\\ Drive/CMPT_CV_lab3/output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYCIXdMZvDYL"
      },
      "source": [
        "'''\n",
        "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
        "# Define a DefaultPredictor\n",
        "'''\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hRCf86KGi5v"
      },
      "source": [
        "'''\n",
        "# Visualize the output for 3 random test samples\n",
        "# TODO: approx 10 lines\n",
        "'''\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_detection_data(\"test\")\n",
        "for d in random.sample(dataset_dicts, 3):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=planes_metadata, \n",
        "                   scale=0.5, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0wRdlcKo6BD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c064ba75-128e-4ea2-fa06-8b6b6f53c461"
      },
      "source": [
        "'''\n",
        "# Use COCOEvaluator and build_detection_train_loader\n",
        "# You can save the output predictions using inference_on_dataset\n",
        "# TODO: approx 5 lines\n",
        "'''\n",
        "\n",
        "evaluator = COCOEvaluator(\"data_detection_val\", output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, \"data_detection_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[11/07 00:04:47 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[11/07 00:04:47 d2.data.common]: \u001b[0mSerializing 30 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[11/07 00:04:47 d2.data.common]: \u001b[0mSerialized dataset takes 3.10 MiB\n",
            "\u001b[32m[11/07 00:04:47 d2.evaluation.evaluator]: \u001b[0mStart inference on 30 batches\n",
            "\u001b[32m[11/07 00:05:06 d2.evaluation.evaluator]: \u001b[0mInference done 11/30. Dataloading: 0.0039 s/iter. Inference: 1.2852 s/iter. Eval: 0.0004 s/iter. Total: 1.2895 s/iter. ETA=0:00:24\n",
            "\u001b[32m[11/07 00:05:12 d2.evaluation.evaluator]: \u001b[0mInference done 16/30. Dataloading: 0.0051 s/iter. Inference: 1.2218 s/iter. Eval: 0.0004 s/iter. Total: 1.2277 s/iter. ETA=0:00:17\n",
            "\u001b[32m[11/07 00:05:18 d2.evaluation.evaluator]: \u001b[0mInference done 20/30. Dataloading: 0.0044 s/iter. Inference: 1.3113 s/iter. Eval: 0.0004 s/iter. Total: 1.3165 s/iter. ETA=0:00:13\n",
            "\u001b[32m[11/07 00:05:24 d2.evaluation.evaluator]: \u001b[0mInference done 24/30. Dataloading: 0.0043 s/iter. Inference: 1.3283 s/iter. Eval: 0.0004 s/iter. Total: 1.3334 s/iter. ETA=0:00:08\n",
            "\u001b[32m[11/07 00:05:30 d2.evaluation.evaluator]: \u001b[0mInference done 28/30. Dataloading: 0.0039 s/iter. Inference: 1.3519 s/iter. Eval: 0.0004 s/iter. Total: 1.3567 s/iter. ETA=0:00:02\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:00:34.042978 (1.361719 s / iter per device, on 1 devices)\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:33 (1.351592 s / iter per device, on 1 devices)\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /content/drive/My Drive/CMPT_CV_lab3/output/coco_instances_results.json\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.02s)\n",
            "creating index...\n",
            "index created!\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.05 seconds.\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.142\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.386\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.061\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.158\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.161\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.110\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.011\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.083\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.187\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.167\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.219\n",
            "\u001b[32m[11/07 00:05:33 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs   |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:------:|:------:|:------:|\n",
            "| 14.213 | 38.554 | 6.053  | 15.787 | 16.074 | 11.011 |\n",
            "OrderedDict([('bbox', {'AP': 14.213204817920975, 'AP50': 38.55438353822251, 'AP75': 6.052737722128967, 'APs': 15.787378477035944, 'APm': 16.07385461791333, 'APl': 11.011294962373977})])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwYvbwcjpKBk"
      },
      "source": [
        "### Improvements\n",
        "\n",
        "For this part, you can bring any improvement which you have by adding new input parameters to the previous functions or defining new functions and variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xdR6KbCZpOlk",
        "outputId": "84688a95-9396-457e-a8a1-7f81440fc68e"
      },
      "source": [
        "'''\n",
        "# Bring any changes and updates regarding the improvement in here\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n# Bring any changes and updates regarding the improvement in here\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98_M4TooqSs2"
      },
      "source": [
        "## Part 2: Semantic Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByAEsMtIPLrO"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCd_pp0XxzZI"
      },
      "source": [
        "training_set = get_detection_data(\"train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwArOMQFAUfT"
      },
      "source": [
        "\n",
        "dicts = []\n",
        "\n",
        "for index in training_set:\n",
        "  filename = index['file_name']\n",
        "  annos =  index['annotations']\n",
        "  height_in =  index['height']\n",
        "  width_in = index['width']\n",
        "\n",
        "  objects = []\n",
        "  img = cv2.imread(filename)\n",
        "\n",
        "  for tot_segs in annos:\n",
        "    bbox = tot_segs['bbox']\n",
        "    x1 = bbox[0]\n",
        "    y1 = bbox[1]\n",
        "    x2 = x1+bbox[2]\n",
        "    y2 = y1+bbox[3]\n",
        "    segmentation = tot_segs['segmentation']\n",
        "    \n",
        "    mask = detectron2.utils.visualizer.GenericMask(segmentation, height_in,width_in).mask\n",
        "\n",
        "    obj_mask = mask[int(y1):int(y2), int(x1):int(x2)]\n",
        "    obj_mask = cv2.resize(obj_mask, (128, 128), interpolation = cv2.INTER_AREA) \n",
        "    cropped = img[int(y1):int(y2), int(x1):int(x2)]\n",
        "    obj_img = cv2.resize(cropped, (128, 128), interpolation = cv2.INTER_AREA) \n",
        "\n",
        "    obj = {\n",
        "        \"mask\": obj_mask,\n",
        "        \"image\" : obj_img\n",
        "    }\n",
        "    objects.append(obj)\n",
        "  inner_dict = {}\n",
        "  inner_dict['filename'] = filename\n",
        "  inner_dict['annos'] = objects\n",
        "  dicts.append(inner_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peQ95zLuIpkk"
      },
      "source": [
        "'''\n",
        "# Write a function that returns the cropped image and corresponding mask regarding the target bounding box\n",
        "# idx is the index of the target bbox in the data\n",
        "# high-resolution image could be passed or could be load from data['file_name']\n",
        "# You can use the mask attribute of detectron2.utils.visualizer.GenericMask \n",
        "#     to convert the segmentation annotations to binary masks\n",
        "# TODO: approx 10 lines\n",
        "'''\n",
        "def get_instance_sample(data, idx, img=None):\n",
        "  for total in range(len(dicts)):\n",
        "    if dicts[total]['filename'] == data['file_name']:\n",
        "      obj_mask = dicts [total] ['annos'] [idx] ['mask']\n",
        "      obj_img = dicts [total] ['annos'] [idx] ['image']\n",
        "      return obj_img, obj_mask\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxrc9X_pjzj-"
      },
      "source": [
        "'''\n",
        "# We have provided a template data loader for your segmentation training\n",
        "# You need to complete the __getitem__() function before running the code\n",
        "# You may also need to add data augmentation or normalization in here\n",
        "'''\n",
        "\n",
        "class PlaneDataset(Dataset):\n",
        "  def __init__(self, set_name, data_list):\n",
        "      self.transforms = transforms.Compose([\n",
        "          transforms.ToTensor(), # Converting the image to tensor and change the image format (Channels-Last => Channels-First)\n",
        "      ])\n",
        "      self.set_name = set_name\n",
        "      self.data = data_list\n",
        "      self.instance_map = []\n",
        "      for i, d in enumerate(self.data):\n",
        "        for j in range(len(d['annotations'])):\n",
        "          self.instance_map.append([i,j])\n",
        "\n",
        "  '''\n",
        "  # you can change the value of length to a small number like 10 for debugging of your training procedure and overfeating\n",
        "  # make sure to use the correct length for the final training\n",
        "  '''\n",
        "  def __len__(self):\n",
        "      return len(self.instance_map)\n",
        "\n",
        "  def numpy_to_tensor(self, img, mask):\n",
        "    if self.transforms is not None:\n",
        "        img = self.transforms(img)\n",
        "    img = torch.tensor(img, dtype=torch.float)\n",
        "    mask = torch.tensor(mask, dtype=torch.float)\n",
        "    return img, mask\n",
        "\n",
        "  '''\n",
        "  # Complete this part by using get_instance_sample function\n",
        "  # make sure to resize the img and mask to a fixed size (for example 128*128)\n",
        "  # you can use \"interpolate\" function of pytorch or \"numpy.resize\"\n",
        "  # TODO: 5 lines\n",
        "  '''\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "        idx = idx.tolist()\n",
        "    idx = self.instance_map[idx]\n",
        "    data = self.data[idx[0]]\n",
        "    img , mask = get_instance_sample(data, idx[1])\n",
        "    img, mask = self.numpy_to_tensor(img, mask)\n",
        "    img = img.reshape((3,128,128))\n",
        "    mask = mask.reshape((1,128,128))\n",
        "    return img, mask\n",
        "\n",
        "def get_plane_dataset(set_name='train', batch_size=2):\n",
        "    my_data_list = DatasetCatalog.get(\"data_detection_{}\".format(set_name))\n",
        "    dataset = PlaneDataset(set_name, my_data_list)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, num_workers=4,\n",
        "                                              pin_memory=True, shuffle=True)\n",
        "    return loader, dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6eH3NKaQQfc"
      },
      "source": [
        "### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeqR3s3dSBPN"
      },
      "source": [
        "'''\n",
        "# convolution module as a template layer consists of conv2d layer, batch normalization, and relu activation\n",
        "'''\n",
        "class conv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, activation=True):\n",
        "        super(conv, self).__init__()\n",
        "        if(activation):\n",
        "          self.layer = nn.Sequential(\n",
        "             nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "             nn.BatchNorm2d(out_ch),\n",
        "             nn.ReLU(inplace=True)\n",
        "          )\n",
        "        else:\n",
        "          self.layer = nn.Sequential(\n",
        "             nn.Conv2d(in_ch, out_ch, 3, padding=1)  \n",
        "             )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "# downsampling module equal to a conv module followed by a max-pool layer\n",
        "'''\n",
        "class down(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super(down, self).__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            conv(in_ch, out_ch),\n",
        "            nn.MaxPool2d(2)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        return x\n",
        "\n",
        "'''\n",
        "# upsampling module equal to a upsample function followed by a conv module\n",
        "'''\n",
        "class up(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, bilinear=False):\n",
        "        super(up, self).__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_ch, in_ch, 2, stride=2)\n",
        "\n",
        "        self.conv = conv(in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.up(x)\n",
        "        y = self.conv(y)\n",
        "        return y\n",
        "\n",
        "'''\n",
        "# the main model which you need to complete by using above modules.\n",
        "# you can also modify the above modules in order to improve your results.\n",
        "'''\n",
        "class MyModel(nn.Module):\n",
        "  \n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "        \n",
        "        # Encoder\n",
        "        self.input_conv = conv(3, 16)\n",
        "        self.down1 = down(16, 32)\n",
        "        self.down2 = down(32, 64)\n",
        "        self.down3 = down(64, 128)\n",
        "        self.down4 = down(128, 256)\n",
        "        self.down5 = down(256, 512)\n",
        "\n",
        "        \n",
        "        # Decoder\n",
        "        self.up1 = up(512, 256)\n",
        "        self.up2 = up(256, 128)\n",
        "        self.up3 = up(128, 64)\n",
        "        self.up4 = up(64, 32)\n",
        "        self.up5 = up(32, 4)\n",
        "        self.output_conv = conv(4, 1, False) # ReLu activation is removed to keep the logits for the loss function\n",
        "\n",
        "    def forward(self, input):\n",
        "      y = self.input_conv(input)\n",
        "      y = self.down1(y)\n",
        "      y = self.down2(y)\n",
        "      y = self.down3(y)\n",
        "      y = self.down4(y)\n",
        "      y = self.down5(y)\n",
        "      y = self.up1(y)\n",
        "      y = self.up2(y)\n",
        "      y = self.up3(y)\n",
        "      y = self.up4(y)\n",
        "      y = self.up5(y)\n",
        "      output = self.output_conv(y)\n",
        "      return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQj86vD9QT_Z"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaZuO4SKSBuF"
      },
      "source": [
        "'''\n",
        "# The following is a basic training procedure to train the network\n",
        "# You need to update the code to get the best performance\n",
        "# TODO: approx ? lines\n",
        "'''\n",
        "\n",
        "# Set the hyperparameters\n",
        "num_epochs = 50 #5\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "weight_decay = 1e-5\n",
        "\n",
        "model = MyModel() # initialize the model\n",
        "model = model.cuda() # move the model to GPU\n",
        "loader, _ = get_plane_dataset('train', batch_size) # initialize data_loader\n",
        "crit = nn.BCEWithLogitsLoss() # Define the loss function\n",
        "optim = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Initialize the optimizer as SGD\n",
        "# start the training procedure\n",
        "for epoch in range(num_epochs):\n",
        "  total_loss = 0\n",
        "  for (img, mask) in tqdm(loader):\n",
        "    img = torch.tensor(img, device=torch.device('cuda'), requires_grad = True).clone().detach()\n",
        "    mask = torch.tensor(mask, device=torch.device('cuda'), requires_grad = True).clone().detach()\n",
        "    # print(dicts)\n",
        "    pred = model(img)\n",
        "    loss = crit(pred, mask)\n",
        "    optim.zero_grad()\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    total_loss += loss.cpu().data\n",
        "  print(\"Epoch: {}, Loss: {}\".format(epoch, total_loss/len(loader)))\n",
        "  torch.save(model.state_dict(), '{}/output/{}_segmentation_model.pth'.format(BASE_DIR, epoch))\n",
        "\n",
        "'''\n",
        "# Saving the final model\n",
        "'''\n",
        "torch.save(model.state_dict(), '{}/output/final_segmentation_model.pth'.format(BASE_DIR))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dyez1fyQYw7"
      },
      "source": [
        "### Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDeViryUSCL2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "6444c5fc898f4caf860c0470b5f06035",
            "1c1c45ea9a7347fd9e280fd2a2959ce7",
            "0d5b5a06a3dc490e9d909c8e8fe7a0dc",
            "2e537c36ce314ddeb21fe7b1b25e9ca9",
            "d29ea9d5cda7434795bb58f62c930722",
            "91aa4b0d63e84c7ba75c87c28ea8226c",
            "366734c9a7d846449155f91a82a87602",
            "25e9c0338bb34e9ab0f39cb314671a56",
            "8f5d8649f2d7427cb4934aaa60187e22",
            "c3069fe2f66840179083bdb0debe32d6",
            "965c355b65c0408cbc5bab6138c5fcb2"
          ]
        },
        "outputId": "83c27674-3055-4a55-f335-7f6972de17a9"
      },
      "source": [
        "'''\n",
        "# Before starting the evaluation, you need to set the model mode to eval\n",
        "# You may load the trained model again, in case if you want to continue your code later\n",
        "# TODO: approx 15 lines\n",
        "'''\n",
        "batch_size = 8\n",
        "model = MyModel().cuda()\n",
        "model.load_state_dict(torch.load('{}/output/final_segmentation_model.pth'.format(BASE_DIR)))\n",
        "model = model.eval() # chaning the model to evaluation mode will fix the bachnorm layers\n",
        "loader, dataset = get_plane_dataset('train', batch_size)\n",
        "\n",
        "total_iou = 0\n",
        "total_images =0 \n",
        "for (img, mask) in tqdm(loader):\n",
        "  with torch.no_grad():\n",
        "    img = img.cuda()\n",
        "    mask = mask.cpu().detach()\n",
        "    mask = torch.unsqueeze(mask,1)\n",
        "    pred = model(img).cpu().detach()\n",
        "\n",
        "    '''\n",
        "    ## Complete the code by obtaining the IoU for each img and print the final Mean IoU\n",
        "    '''\n",
        "    for i in range(img.shape[0]):\n",
        "      predicted_image = pred[i]\n",
        "      predicted_image = nn.Sigmoid()(predicted_image)[0]\n",
        "      predicted_image = np.array(predicted_image)\n",
        "      predicted_image = np.rint(predicted_image)\n",
        "\n",
        "      masked = mask[i]\n",
        "      ground_truth = nn.Sigmoid()(masked)[0]\n",
        "      ground_truth = np.array(ground_truth)\n",
        "      ground_truth = np.rint(ground_truth)\n",
        "\n",
        "      Inter_o_union = np.sum(np.logical_and (ground_truth, predicted_image)) / np.sum(np.logical_or (ground_truth, predicted_image))\n",
        "      total_images+=1\n",
        "      total_iou += Inter_o_union\n",
        "\n",
        "\n",
        "mean = total_iou/total_images\n",
        "\n",
        "print(\"\\n #images: {}, Mean IoU: {}\".format(total_images,mean))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6444c5fc898f4caf860c0470b5f06035",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/837 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " #images: 6694, Mean IoU: 0.812196756916907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbmTj9JICiKz"
      },
      "source": [
        "'''\n",
        "# Visualize 3 sample outputs\n",
        "# TODO: approx 5 lines\n",
        "'''\n",
        "\n",
        "loader, dataset = get_plane_dataset('train', 3)\n",
        "\n",
        "img, mask = next(iter(loader))\n",
        "img = img.cuda()\n",
        "mask = mask.cpu().detach()\n",
        "pred = model(img).cpu().detach()\n",
        "for i in range(3):\n",
        "  image = img[i].cpu()\n",
        "  im = transforms.ToPILImage()(image)\n",
        "  cv2_imshow(np.array(im))\n",
        "  prediction = np.rint(np.array(nn.Sigmoid()(pred[i])[0]))*255\n",
        "  masked = np.array(mask[i])[0]*255\n",
        "  cv2_imshow(prediction)\n",
        "  cv2_imshow(masked)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "navoiGdrqaZT"
      },
      "source": [
        "## Part 3: Instance Segmentation\n",
        "\n",
        "In this part, you need to obtain the instance segmentation results for the test data by using the trained segmentation model in the previous part and the detection model in Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBwk33DGBowP"
      },
      "source": [
        "### Get Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crrZ8TG-Ot2J"
      },
      "source": [
        "'''\n",
        "# Define a new function to obtain the prediction mask by passing a sample data\n",
        "# For this part, you need to use all the previous parts (predictor, get_instance_sample, data preprocessings, etc)\n",
        "# It is better to keep everything (as well as the output of this funcion) on gpu as tensors to speed up the operations.\n",
        "# pred_mask is the instance segmentation result and should have different values for different planes.\n",
        "# TODO: approx 35 lines\n",
        "'''\n",
        "\n",
        "def sigmoid_func(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def get_prediction_mask(data):\n",
        "  # print(data)\n",
        "  filename = data['file_name']\n",
        "  height = data['height']\n",
        "  width = data['width']\n",
        "  annotations = data['annotations']\n",
        "  img = cv2.imread(filename)\n",
        "\n",
        "  prediction_mask = np.zeros([height, width])\n",
        "  ground_truth_mask = np.zeros([height, width])\n",
        "\n",
        "  if len(annotations) == 0:\n",
        "    #test code\n",
        "    bbox_finding = predictor(img)['instances']\n",
        "    for i in range(len(bbox_finding)):\n",
        "      bbox = np.array(np.floor(list(bbox_finding[i]._fields['pred_boxes'])[0].cpu().numpy()), dtype=np.uint32)\n",
        "      x1 = bbox[0]\n",
        "      y1 = bbox[1]\n",
        "      x2 = bbox[2]\n",
        "      y2 = bbox[3]\n",
        "\n",
        "      pred = img[int(y1):int(y2), int(x1):int(x2)]\n",
        "      pred = cv2.resize(pred, (128, 128), interpolation = cv2.INTER_AREA)\n",
        "      pred = torch.unsqueeze(torch.tensor(transforms.ToTensor()(pred), device=torch.device('cuda')),0)\n",
        "      \n",
        "      pred = model(pred).cpu().detach().numpy()[0]\n",
        "      pred = cv2.resize(pred[0], (int(x2)-int(x1), int(y2)-int(y1)), interpolation = cv2.INTER_AREA)\n",
        "      pred = sigmoid_func(pred)\n",
        "      pred = np.array(pred)\n",
        "      pred = np.rint(pred)\n",
        "      pred = pred*(i+1)\n",
        "\n",
        "    \n",
        "      mask = prediction_mask[int(y1):int(y2), int(x1):int(x2)] \n",
        "      mask[mask == 0] = 10000\n",
        "\n",
        "      covering_pm = np.minimum(mask, pred)\n",
        "      covering_pm[covering_pm == 1000] = 0\n",
        "      prediction_mask[int(y1):int(y2), int(x1):int(x2)] = covering_pm\n",
        "\n",
        "    for i,j in enumerate(annotations):\n",
        "      bbox = anno['bbox']\n",
        "      x1 = bbox[0]\n",
        "      y1 = bbox[1]\n",
        "      x2 = x1+bbox[2]\n",
        "      y2 = y1+bbox[3]\n",
        "\n",
        "      real_mask = detectron2.utils.visualizer.GenericMask(anno['segmentation'], height, width).mask\n",
        "      covering_tm = np.maximum(ground_truth_mask[int(y1):int(y2), int(x1):int(x2)],real_mask[int(y1):int(y2), int(x1):int(x2)]*(idx+1))\n",
        "      ground_truth_mask[int(y1):int(y2), int(x1):int(x2)] = covering_tm\n",
        "\n",
        "\n",
        "    \n",
        "  else:\n",
        "    #train code\n",
        "    for idx, anno in enumerate(annotations):\n",
        "      bbox = anno['bbox']\n",
        "      x1 = bbox[0]\n",
        "      y1 = bbox[1]\n",
        "      x2 = x1+bbox[2]\n",
        "      y2 = y1+bbox[3]\n",
        "\n",
        "      pred = img[int(y1):int(y2), int(x1):int(x2)]\n",
        "      pred = cv2.resize(pred, (128, 128), interpolation = cv2.INTER_AREA)\n",
        "      pred = torch.unsqueeze(torch.tensor(transforms.ToTensor()(pred), device=torch.device('cuda')),0)\n",
        "      \n",
        "      pred = model(pred).cpu().detach().numpy()[0]\n",
        "      pred = cv2.resize(pred[0], (int(x2-x1), int(y2-y1)), interpolation = cv2.INTER_AREA)\n",
        "      pred = sigmoid_func(pred)\n",
        "      pred = np.array(pred)\n",
        "      pred = np.rint(pred)\n",
        "      pred = pred*(idx+1)\n",
        "\n",
        "    \n",
        "      mask = prediction_mask[int(y1):int(y2), int(x1):int(x2)] \n",
        "      mask[mask == 0] = 10000\n",
        "\n",
        "      covering_pm = np.minimum(mask, pred)\n",
        "      covering_pm[covering_pm == 1000] = 0\n",
        "      prediction_mask[int(y1):int(y2), int(x1):int(x2)] = covering_pm\n",
        "      \n",
        "\n",
        "\n",
        "      real_mask = detectron2.utils.visualizer.GenericMask(anno['segmentation'], height, width).mask\n",
        "      covering_tm = np.maximum(ground_truth_mask[int(y1):int(y2), int(x1):int(x2)],real_mask[int(y1):int(y2), int(x1):int(x2)]*(idx+1))\n",
        "      ground_truth_mask[int(y1):int(y2), int(x1):int(x2)] = covering_tm\n",
        "\n",
        "  gt_mask = ground_truth_mask\n",
        "  gt_mask = torch.tensor(gt_mask,device=torch.device('cuda'))\n",
        "  pred_mask = prediction_mask\n",
        "  pred_mask = torch.tensor(prediction_mask, device=torch.device('cuda'))\n",
        "\n",
        "  return img, gt_mask, pred_mask # gt_mask could be all zero when the ground truth is not given.\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc7TSK6EBi9u"
      },
      "source": [
        "### Visualization and Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7T2YX8MBiGO"
      },
      "source": [
        "'''\n",
        "# Visualise the output prediction as well as the GT Mask and Input image for a sample input\n",
        "# TODO: approx 10 lines\n",
        "'''\n",
        "\n",
        "training_set = get_detection_data('train')\n",
        "# length = len(training_set)\n",
        "random = np.random.randint(0,167,3)\n",
        "for i in random:\n",
        "\n",
        "  img, gt_mask, pred_mask = get_prediction_mask(training_set[i])\n",
        "  pred_mask = pred_mask.cpu().numpy()\n",
        "  value_pred = pred_mask.max()\n",
        "  value_pred = 255./value_pred\n",
        "  pred_mask = pred_mask * value_pred\n",
        "\n",
        "  gt_mask = gt_mask.cpu().numpy()\n",
        "  value_gt = gt_mask.max()\n",
        "  value_gt = 255./value_gt\n",
        "  gt_mask = gt_mask * value_gt\n",
        "\n",
        "  cv2_imshow(cv2.resize(img, (img.shape[1], img.shape[0]), interpolation = cv2.INTER_AREA))\n",
        "  cv2_imshow(cv2.resize(gt_mask, (gt_mask.shape[1], gt_mask.shape[0]), interpolation = cv2.INTER_AREA))\n",
        "  cv2_imshow(cv2.resize(pred_mask, (pred_mask.shape[1], pred_mask.shape[0]), interpolation = cv2.INTER_AREA))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPo_03up-g_f"
      },
      "source": [
        "'''\n",
        "# ref: https://www.kaggle.com/rakhlin/fast-run-length-encoding-python\n",
        "# https://www.kaggle.com/c/airbus-ship-detection/overview/evaluation\n",
        "'''\n",
        "def rle_encoding(x):\n",
        "    '''\n",
        "    x: pytorch tensor on gpu, 1 - mask, 0 - background\n",
        "    Returns run length as list\n",
        "    '''\n",
        "    dots = torch.where(torch.flatten(x.long())==1)[0]\n",
        "    if(len(dots)==0):\n",
        "      return []\n",
        "    inds = torch.where(dots[1:]!=dots[:-1]+1)[0]+1\n",
        "    inds = torch.cat((torch.tensor([0], device=torch.device('cuda'), dtype=torch.long), inds))\n",
        "    tmpdots = dots[inds]\n",
        "    inds = torch.cat((inds, torch.tensor([len(dots)], device=torch.device('cuda'))))\n",
        "    inds = inds[1:] - inds[:-1]\n",
        "    runs = torch.cat((tmpdots, inds)).reshape((2,-1))\n",
        "    runs = torch.flatten(torch.transpose(runs, 0, 1)).cpu().data.numpy()\n",
        "    return ' '.join([str(i) for i in runs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv0rab2LJev-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "7b3925afd1744aa6b98818290fbcc7aa",
            "ae7dba3036dd45968e336f584911dcff",
            "57951810118e4bb5a25a9378de3e3918",
            "f1c7a470ad8b4cad92b3762885936e9f",
            "6eb570df63fe41bbb3c903b8aeeb6cae",
            "c6703f8e19984d6b97d64f625e53183f",
            "d67a4a8ae01046b1a954a4dc349fa43d",
            "ed300f57efdd4ab7833e3b6ebf034b6e",
            "5ed0bba00fd140dc96c4683f8e75fa9a",
            "cfc861d64ba745d8bdeb87594d75cf81",
            "80fb3cc6110d4f68878eb434f5b2877c",
            "a02398ad82134f08807f6ea242b4c79a",
            "9c027802ae8f4bc6a6d2b018db48bc0d",
            "8f61f0f140494de194ac51e71d0c05b0",
            "92526b7a9d514ed0bafa928c4769fb1c",
            "ea31cc7899b94be4b648ae064838b684",
            "c6a6d527d2f34885b90aa4348dc77ea4",
            "53682e77e13d4691a39aad2174c8d152",
            "31ebe21e545844db938852cc759e2de8",
            "9bfdd9f763eb4d24b995649046f294ca",
            "9434093f4528454f95910344b6f5e52c",
            "49ceffab110d41cf854975f6e062965b",
            "40ca5847e20b4068b635db32f77d2e3c",
            "145638c4a6414cec874831ed057ae549",
            "080e286c79b644c2a74a392791e66a58",
            "fdad7ca005ce43f5a0190a0eec89e533",
            "3bc0f535197043009280ce9a119cac3b",
            "5f3693a1980b494bb67b44f67761d4e3",
            "2475cc85c4c945c085f22b4774bec488",
            "9616236938da4a7da9dbb8f055173cb1",
            "f43b2567dbc84be8822819b0c07405f7",
            "b553a63650d242fd99966453416b2702",
            "5f3f9f477b7f4da29ea6291dd81afcdb"
          ]
        },
        "outputId": "1c2d45e4-d530-4208-beb3-bf92925aec86"
      },
      "source": [
        "'''\n",
        "# You need to upload the csv file on kaggle\n",
        "# The speed of your code in the previous parts highly affects the running time of this part\n",
        "'''\n",
        "\n",
        "preddic = {\"ImageId\": [], \"EncodedPixels\": []}\n",
        "\n",
        "'''\n",
        "# Writing the predictions of the training set\n",
        "'''\n",
        "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('train'))\n",
        "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
        "  sample = my_data_list[i]\n",
        "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
        "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
        "  inds = torch.unique(pred_mask)\n",
        "  if(len(inds)==1):\n",
        "    preddic['ImageId'].append(sample['image_id'])\n",
        "    preddic['EncodedPixels'].append([])\n",
        "  else:\n",
        "    for index in inds:\n",
        "      if(index == 0):\n",
        "        continue\n",
        "      tmp_mask = (pred_mask==index)\n",
        "      encPix = rle_encoding(tmp_mask)\n",
        "      preddic['ImageId'].append(sample['image_id'])\n",
        "      preddic['EncodedPixels'].append(encPix)\n",
        "\n",
        "\n",
        "'''\n",
        "# Writing the predictions of the validation set\n",
        "'''\n",
        "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('val'))\n",
        "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
        "  sample = my_data_list[i]\n",
        "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
        "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
        "  inds = torch.unique(pred_mask)\n",
        "  if(len(inds)==1):\n",
        "    preddic['ImageId'].append(sample['image_id'])\n",
        "    preddic['EncodedPixels'].append([])\n",
        "  else:\n",
        "    for index in inds:\n",
        "      if(index == 0):\n",
        "        continue\n",
        "      tmp_mask = (pred_mask==index)\n",
        "      encPix = rle_encoding(tmp_mask)\n",
        "      preddic['ImageId'].append(sample['image_id'])\n",
        "      preddic['EncodedPixels'].append(encPix)\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# Writing the predictions of the test set\n",
        "'''\n",
        "\n",
        "my_data_list = DatasetCatalog.get(\"data_detection_{}\".format('test'))\n",
        "for i in tqdm(range(len(my_data_list)), position=0, leave=True):\n",
        "  sample = my_data_list[i]\n",
        "  sample['image_id'] = sample['file_name'].split(\"/\")[-1][:-4]\n",
        "  img, true_mask, pred_mask = get_prediction_mask(sample)\n",
        "  inds = torch.unique(pred_mask)\n",
        "  if(len(inds)==1):\n",
        "    preddic['ImageId'].append(sample['image_id'])\n",
        "    preddic['EncodedPixels'].append([])\n",
        "  else:\n",
        "    for j, index in enumerate(inds):\n",
        "      if(index == 0):\n",
        "        continue\n",
        "      tmp_mask = (pred_mask==index).double()\n",
        "      encPix = rle_encoding(tmp_mask)\n",
        "      preddic['ImageId'].append(sample['image_id'])\n",
        "      preddic['EncodedPixels'].append(encPix)\n",
        "\n",
        "pred_file = open(\"{}/pred.csv\".format(BASE_DIR), 'w')\n",
        "pd.DataFrame(preddic).to_csv(pred_file, index=False)\n",
        "pred_file.close()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b3925afd1744aa6b98818290fbcc7aa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/168 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:76: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a02398ad82134f08807f6ea242b4c79a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40ca5847e20b4068b635db32f77d2e3c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/72 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7nN4SduqrpI"
      },
      "source": [
        "## Part 4: Mask R-CNN\n",
        "\n",
        "For this part you need to follow a same procedure to part 2 with the configs of Mask R-CNN, other parts are generally the same as part 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axWf7drKNXYd"
      },
      "source": [
        "### Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yC1FDCcQN2LH"
      },
      "source": [
        "DatasetCatalog.remove('data_detection_train')\n",
        "DatasetCatalog.remove('data_detection_test')\n",
        "DatasetCatalog.remove('data_detection_val')\n",
        "\n",
        "for d in [\"train\", \"val\",\"test\"]:\n",
        "    # print(d)\n",
        "    DatasetCatalog.register(\"data_detection_\" + d, lambda d=d: get_detection_data(d))\n",
        "    MetadataCatalog.get(\"data_detection_\" + d).set(thing_classes=[\"planes\"])\n",
        "planes_metadata = MetadataCatalog.get(\"data_detection_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG5slAhQNjE7"
      },
      "source": [
        "### Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KG1pnKLMOcjS"
      },
      "source": [
        "'''\n",
        "# Set the configs for the detection part in here.\n",
        "# TODO: approx 15 lines\n",
        "'''\n",
        "cfg = get_cfg()\n",
        "cfg.OUTPUT_DIR = \"{}/output/\".format(BASE_DIR)\n",
        "\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"data_detection_train\")\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
        "cfg.SOLVER.MAX_ITER = 500 #500   \n",
        "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 512   # faster, and good enough for this toy dataset (default: 512)\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
        "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
        "\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7ifeV1sNvtt"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tc4K0Nz5OeKk"
      },
      "source": [
        "trainer = DefaultTrainer(cfg) \n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHa0duLf2nvv"
      },
      "source": [
        "'''\n",
        "# After training the model, you need to update cfg.MODEL.WEIGHTS\n",
        "# Define a DefaultPredictor\n",
        "'''\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.6   # set a custom testing threshold\n",
        "predictor = DefaultPredictor(cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "298QruFnNxyn"
      },
      "source": [
        "### Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KcGwV5-9Oetp"
      },
      "source": [
        "'''\n",
        "# Visualize the output for 3 random test samples\n",
        "# TODO: approx 10 lines\n",
        "'''\n",
        "\n",
        "from detectron2.utils.visualizer import ColorMode\n",
        "dataset_dicts = get_detection_data(\"val\")\n",
        "# print(len(dataset_dicts))\n",
        "for d in random.sample(dataset_dicts, 3):    \n",
        "    im = cv2.imread(d[\"file_name\"])\n",
        "    outputs = predictor(im)  # format is documented at https://detectron2.readthedocs.io/tutorials/models.html#model-output-format\n",
        "    v = Visualizer(im[:, :, ::-1],\n",
        "                   metadata=planes_metadata, \n",
        "                   scale=0.5, \n",
        "                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n",
        "    )\n",
        "    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "    cv2_imshow(out.get_image()[:, :, ::-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_3wS2BFLLGp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c097051a-debf-47d8-d0b5-9ef029c3867d"
      },
      "source": [
        "'''\n",
        "# Use COCOEvaluator and build_detection_train_loader\n",
        "# You can save the output predictions using inference_on_dataset\n",
        "# TODO: approx 5 lines\n",
        "'''\n",
        "\n",
        "evaluator = COCOEvaluator(\"data_detection_val\", output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, \"data_detection_val\")\n",
        "print(inference_on_dataset(predictor.model, val_loader, evaluator))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m[11/07 05:53:44 d2.evaluation.coco_evaluation]: \u001b[0mTrying to convert 'data_detection_val' to COCO format ...\n",
            "\u001b[32m[11/07 05:53:44 d2.data.datasets.coco]: \u001b[0mConverting annotations of dataset 'data_detection_val' to COCO format ...)\n",
            "\u001b[32m[11/07 05:53:44 d2.data.datasets.coco]: \u001b[0mConverting dataset dicts into COCO format\n",
            "\u001b[32m[11/07 05:53:44 d2.data.datasets.coco]: \u001b[0mConversion finished, #images: 30, #annotations: 1882\n",
            "\u001b[32m[11/07 05:53:44 d2.data.datasets.coco]: \u001b[0mCaching COCO format annotations at '/content/drive/My Drive/CMPT_CV_lab3/output/data_detection_val_coco_format.json' ...\n",
            "\u001b[32m[11/07 05:53:46 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
            "\u001b[36m|  category  | #instances   |\n",
            "|:----------:|:-------------|\n",
            "|   planes   | 1882         |\n",
            "|            |              |\u001b[0m\n",
            "\u001b[32m[11/07 05:53:46 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
            "\u001b[32m[11/07 05:53:46 d2.data.common]: \u001b[0mSerializing 30 elements to byte tensors and concatenating them all ...\n",
            "\u001b[32m[11/07 05:53:46 d2.data.common]: \u001b[0mSerialized dataset takes 3.50 MiB\n",
            "\u001b[32m[11/07 05:53:46 d2.evaluation.evaluator]: \u001b[0mStart inference on 30 batches\n",
            "\u001b[32m[11/07 05:54:45 d2.evaluation.evaluator]: \u001b[0mInference done 11/30. Dataloading: 0.0491 s/iter. Inference: 0.7065 s/iter. Eval: 5.7419 s/iter. Total: 6.4975 s/iter. ETA=0:02:03\n",
            "\u001b[32m[11/07 05:55:32 d2.evaluation.evaluator]: \u001b[0mInference done 12/30. Dataloading: 0.0431 s/iter. Inference: 0.8946 s/iter. Eval: 11.3477 s/iter. Total: 12.2860 s/iter. ETA=0:03:41\n",
            "\u001b[32m[11/07 05:55:42 d2.evaluation.evaluator]: \u001b[0mInference done 15/30. Dataloading: 0.0794 s/iter. Inference: 0.7728 s/iter. Eval: 8.7620 s/iter. Total: 9.6154 s/iter. ETA=0:02:24\n",
            "\u001b[32m[11/07 05:55:50 d2.evaluation.evaluator]: \u001b[0mInference done 19/30. Dataloading: 0.0582 s/iter. Inference: 0.6640 s/iter. Eval: 6.7115 s/iter. Total: 7.4347 s/iter. ETA=0:01:21\n",
            "\u001b[32m[11/07 05:56:10 d2.evaluation.evaluator]: \u001b[0mInference done 22/30. Dataloading: 0.0623 s/iter. Inference: 0.6474 s/iter. Eval: 6.6272 s/iter. Total: 7.3379 s/iter. ETA=0:00:58\n",
            "\u001b[32m[11/07 05:56:16 d2.evaluation.evaluator]: \u001b[0mInference done 25/30. Dataloading: 0.0539 s/iter. Inference: 0.6101 s/iter. Eval: 5.8723 s/iter. Total: 6.5373 s/iter. ETA=0:00:32\n",
            "\u001b[32m[11/07 05:56:39 d2.evaluation.evaluator]: \u001b[0mInference done 27/30. Dataloading: 0.0499 s/iter. Inference: 0.6145 s/iter. Eval: 6.2878 s/iter. Total: 6.9532 s/iter. ETA=0:00:20\n",
            "\u001b[32m[11/07 05:56:58 d2.evaluation.evaluator]: \u001b[0mInference done 29/30. Dataloading: 0.0459 s/iter. Inference: 0.6158 s/iter. Eval: 6.5046 s/iter. Total: 7.1672 s/iter. ETA=0:00:07\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.evaluator]: \u001b[0mInference done 30/30. Dataloading: 0.0444 s/iter. Inference: 0.6151 s/iter. Eval: 6.6432 s/iter. Total: 7.3037 s/iter. ETA=0:00:00\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:03:02.695206 (7.307808 s / iter per device, on 1 devices)\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:00:15 (0.615063 s / iter per device, on 1 devices)\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to /content/drive/My Drive/CMPT_CV_lab3/output/coco_instances_results.json\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
            "Loading and preparing results...\n",
            "DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.06 seconds.\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.00 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.168\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.280\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.178\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.073\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.263\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.541\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.078\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.190\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.071\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.669\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
            "|   AP   |  AP50  |  AP75  |  APs  |  APm   |  APl   |\n",
            "|:------:|:------:|:------:|:-----:|:------:|:------:|\n",
            "| 16.838 | 28.018 | 17.777 | 7.333 | 26.288 | 54.076 |\n",
            "Loading and preparing results...\n",
            "DONE (t=0.01s)\n",
            "creating index...\n",
            "index created!\n",
            "\u001b[32m[11/07 05:57:08 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *segm*\n",
            "\u001b[32m[11/07 05:57:09 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 0.27 seconds.\n",
            "\u001b[32m[11/07 05:57:09 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
            "\u001b[32m[11/07 05:57:09 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.01 seconds.\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.150\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.012\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.018\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.044\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.252\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.004\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.028\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.053\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.024\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.065\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.289\n",
            "\u001b[32m[11/07 05:57:09 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for segm: \n",
            "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
            "|:-----:|:------:|:------:|:-----:|:-----:|:------:|\n",
            "| 3.979 | 15.033 | 1.173  | 1.774 | 4.441 | 25.165 |\n",
            "OrderedDict([('bbox', {'AP': 16.838123079673075, 'AP50': 28.01757456538375, 'AP75': 17.776661848616808, 'APs': 7.333155264052806, 'APm': 26.287574110793383, 'APl': 54.075989656876914}), ('segm', {'AP': 3.978711934804055, 'AP50': 15.03285112475608, 'AP75': 1.1734506784011733, 'APs': 1.7737573476874535, 'APm': 4.440758999248592, 'APl': 25.16545613351212})])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAAj-ntl4kL2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}